{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/George-Okello/Ambiguity/blob/main/Ambiguity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMmcu1zoHYrS"
      },
      "outputs": [],
      "source": [
        "# Install HuggingFace Transformers and Datasets libraries\n",
        "!pip install transformers datasets -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmB0vnycTZjL"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import torch\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True  # less efficient but reproducible\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "# Load the WSC dataset\n",
        "dataset = load_dataset(\"super_glue\", \"wsc.fixed\")\n",
        "dataset = dataset[\"validation\"]  # Using the validation set for testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTNSZZpdTdPR"
      },
      "outputs": [],
      "source": [
        "# View a sample\n",
        "sample = dataset[0]\n",
        "for k, v in sample.items():\n",
        "    print(f\"{k}: {v}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LauulyeWTfP2"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbT5KzcOT2Tn"
      },
      "outputs": [],
      "source": [
        "from torch.nn.functional import softmax\n",
        "\n",
        "def score_sentence(sentence, target):\n",
        "    # Tokenize and locate the token span\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "    # Get score for entire sentence (log prob of each token)\n",
        "    input_ids = inputs.input_ids[0]\n",
        "    log_probs = softmax(logits[0], dim=-1)\n",
        "    token_probs = [log_probs[i, input_ids[i]].item() for i in range(len(input_ids))]\n",
        "    return sum(torch.log(torch.tensor(token_probs))).item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDasa-nNT78f"
      },
      "outputs": [],
      "source": [
        "text = sample[\"text\"]\n",
        "span1 = sample[\"span1_text\"]\n",
        "span2 = sample[\"span2_text\"]\n",
        "\n",
        "# Replace ambiguous pronoun with noun\n",
        "text_with_sub = text.replace(span1, span2)\n",
        "\n",
        "# Score both\n",
        "original_score = score_sentence(text, span1)\n",
        "sub_score = score_sentence(text_with_sub, span2)\n",
        "\n",
        "print(f\"Original: {text}\")\n",
        "print(f\"Modified: {text_with_sub}\")\n",
        "print(f\"Score with pronoun: {original_score:.2f}\")\n",
        "print(f\"Score with candidate: {sub_score:.2f}\")\n",
        "\n",
        "# Prediction\n",
        "prediction = int(sub_score > original_score)\n",
        "print(f\"Predicted label: {prediction}, True label: {sample['label']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1Zan-3EUAQY"
      },
      "outputs": [],
      "source": [
        "correct = 0\n",
        "\n",
        "for i in range(len(dataset)):\n",
        "    ex = dataset[i]\n",
        "    if ex['span1_text'] not in ex['text']:\n",
        "        continue  # Skip malformed entries\n",
        "    text = ex['text']\n",
        "    span1 = ex['span1_text']\n",
        "    span2 = ex['span2_text']\n",
        "    label = ex['label']\n",
        "\n",
        "    try:\n",
        "        text_with_sub = text.replace(span1, span2)\n",
        "        original_score = score_sentence(text, span1)\n",
        "        sub_score = score_sentence(text_with_sub, span2)\n",
        "        prediction = int(sub_score > original_score)\n",
        "        correct += (prediction == label)\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "accuracy = correct / len(dataset)\n",
        "print(f\"\\n📊 Accuracy: {accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAuPteGMUjRP"
      },
      "source": [
        "Evaluate RoBERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHBopx4VUlZH"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "import torch\n",
        "from torch.nn.functional import softmax\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load model\n",
        "roberta_tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")\n",
        "roberta_model = AutoModelForMaskedLM.from_pretrained(\"roberta-large\").eval()\n",
        "\n",
        "# Load WSC data\n",
        "wsc = load_dataset(\"super_glue\", \"wsc.fixed\")[\"validation\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vHzKMeOVQwt"
      },
      "outputs": [],
      "source": [
        "def roberta_score_sentence(sentence):\n",
        "    inputs = roberta_tokenizer(sentence, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        logits = roberta_model(**inputs).logits\n",
        "    input_ids = inputs.input_ids[0]\n",
        "    probs = softmax(logits[0], dim=-1)\n",
        "    token_probs = [probs[i, input_ids[i]].item() for i in range(len(input_ids))]\n",
        "    return sum(torch.log(torch.tensor(token_probs))).item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgSWCKiVVT-f"
      },
      "outputs": [],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for ex in wsc:\n",
        "    try:\n",
        "        t = ex['text']\n",
        "        span1 = ex['span1_text']\n",
        "        span2 = ex['span2_text']\n",
        "        if span1 not in t or span2 not in t:\n",
        "            continue\n",
        "        modified = t.replace(span2, span1)\n",
        "        orig_score = roberta_score_sentence(t)\n",
        "        mod_score = roberta_score_sentence(modified)\n",
        "        pred = int(mod_score > orig_score)\n",
        "        if pred == ex['label']:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "    except Exception:\n",
        "        continue\n",
        "\n",
        "print(f\"📊 RoBERTa Accuracy: {correct / total * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFmCBDT1WDJy"
      },
      "outputs": [],
      "source": [
        "def masked_fill(sentence, masked_word):\n",
        "    masked = sentence.replace(masked_word, roberta_tokenizer.mask_token)\n",
        "    inputs = roberta_tokenizer(masked, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        logits = roberta_model(**inputs).logits\n",
        "    mask_token_index = (inputs.input_ids == roberta_tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
        "    predicted_ids = logits[0, mask_token_index].topk(5).indices[0].tolist()\n",
        "    return [roberta_tokenizer.decode([idx]).strip() for idx in predicted_ids]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlFFFh5eWGqY"
      },
      "outputs": [],
      "source": [
        "example = wsc[0]\n",
        "masked_fill(example[\"text\"], example[\"span2_text\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "os5kGouHWzUH"
      },
      "source": [
        "Evaluate DeBERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4EonDvj9XE2h"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "import torch\n",
        "from torch.nn.functional import softmax\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load model\n",
        "deberta_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
        "deberta_model = AutoModelForMaskedLM.from_pretrained(\"microsoft/deberta-v3-base\")\n",
        "\n",
        "# Load WSC data\n",
        "wsc = load_dataset(\"super_glue\", \"wsc.fixed\")[\"validation\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGNz6WdJXSa4"
      },
      "outputs": [],
      "source": [
        "def deberta_score_sentence(sentence):\n",
        "    inputs = deberta_tokenizer(sentence, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        logits = deberta_model(**inputs).logits\n",
        "    input_ids = inputs.input_ids[0]\n",
        "    probs = softmax(logits[0], dim=-1)\n",
        "    token_probs = [probs[i, input_ids[i]].item() for i in range(len(input_ids))]\n",
        "    return sum(torch.log(torch.tensor(token_probs))).item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgjiG3QZXyhF"
      },
      "outputs": [],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for ex in wsc:\n",
        "    try:\n",
        "        t = ex['text']\n",
        "        span1 = ex['span1_text']\n",
        "        span2 = ex['span2_text']\n",
        "        if span1 not in t or span2 not in t:\n",
        "            continue\n",
        "        modified = t.replace(span2, span1)\n",
        "        orig_score = deberta_score_sentence(t)\n",
        "        mod_score = deberta_score_sentence(modified)\n",
        "        pred = int(mod_score > orig_score)\n",
        "        if pred == ex['label']:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "    except Exception:\n",
        "        continue\n",
        "\n",
        "print(f\"📊 DeBERTa Accuracy: {correct / total * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MzlSXeXbxJ0"
      },
      "outputs": [],
      "source": [
        "example = wsc[0]\n",
        "masked_fill(example[\"text\"], example[\"span2_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "addAYO9EZuhw"
      },
      "source": [
        "Use [MASK] Token to Probe Language Understanding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGTz2_OGbBzJ"
      },
      "outputs": [],
      "source": [
        "def get_mask_predictions(sentence, target_word, model, tokenizer, top_k=5):\n",
        "    # Replace the target word with the model's mask token\n",
        "    if target_word not in sentence:\n",
        "        raise ValueError(\"Target word not in sentence.\")\n",
        "\n",
        "    masked_sentence = sentence.replace(target_word, tokenizer.mask_token, 1)\n",
        "    inputs = tokenizer(masked_sentence, return_tensors=\"pt\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs).logits\n",
        "\n",
        "    mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
        "    top_k_ids = outputs[0, mask_token_index].topk(top_k, dim=-1).indices[0].tolist()\n",
        "    predictions = [tokenizer.decode([idx]).strip() for idx in top_k_ids]\n",
        "\n",
        "    return masked_sentence, predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaRpT0Y1bGyU"
      },
      "outputs": [],
      "source": [
        "sample = wsc[0]  # Use the first WSC example\n",
        "sentence = sample[\"text\"]\n",
        "target = sample[\"span2_text\"]\n",
        "\n",
        "masked, guesses = get_mask_predictions(sentence, target, roberta_model, roberta_tokenizer)\n",
        "print(\"Masked sentence:\", masked)\n",
        "print(\"Top predictions:\", guesses)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50r22fvMd2lt"
      },
      "source": [
        "DeBERTa is better at reasoning about relationships between entities, while RoBERTa is better at predicting what words should appear in contex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrEPywGIbj8l"
      },
      "source": [
        "Fine-Tuning BERT on WSC (Binary Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_899a2ycbonp"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YowyhouniZ37"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQYEroy0icm0"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"super_glue\", \"wsc\")\n",
        "train_data = dataset['train']\n",
        "val_data = dataset['validation']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6VxuQJrie8x"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-base')\n",
        "special_tokens = ['<s1>', '</s1>', '<s2>', '</s2>']\n",
        "tokenizer.add_tokens(special_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HAQw2Cmi6nn"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "class WSCDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_len=128):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        example = self.data[idx]\n",
        "        sentence = example['text']\n",
        "        span1 = example['span1_text']\n",
        "        span2 = example['span2_text']\n",
        "        label = example['label']\n",
        "\n",
        "        # Add special tokens to help model recognize spans\n",
        "        marked_sentence = sentence.replace(span1, f\"<s1>{span1}</s1>\")\n",
        "        marked_sentence = marked_sentence.replace(span2, f\"<s2>{span2}</s2>\")\n",
        "\n",
        "        # Tokenize\n",
        "        encoding = self.tokenizer(\n",
        "            marked_sentence,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_len,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        input_ids = encoding['input_ids'].squeeze()\n",
        "        attention_mask = encoding['attention_mask'].squeeze()\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFFjQO6Ti98A"
      },
      "outputs": [],
      "source": [
        "class WSCClassifier(nn.Module):\n",
        "    def __init__(self, tokenizer_len):\n",
        "        super().__init__()\n",
        "        self.deberta = AutoModel.from_pretrained('microsoft/deberta-v3-base')\n",
        "        # Resize embeddings for new special tokens\n",
        "        self.deberta.resize_token_embeddings(tokenizer_len)\n",
        "        self.classifier = nn.Linear(self.deberta.config.hidden_size, 2)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        # Use [CLS] token (position 0) for classification\n",
        "        cls_token = outputs.last_hidden_state[:, 0, :]\n",
        "        cls_token = self.dropout(cls_token)\n",
        "        return self.classifier(cls_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3oTcGMoIjA6H"
      },
      "outputs": [],
      "source": [
        "train_dataset = WSCDataset(train_data, tokenizer)\n",
        "val_dataset = WSCDataset(val_data, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgO-z4pml64P"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "# Clear CUDA cache\n",
        "import torch\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twI2HKDXl-gi"
      },
      "outputs": [],
      "source": [
        "class WSCClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size=None):  # Add parameter if needed\n",
        "        super().__init__()\n",
        "        self.deberta = AutoModel.from_pretrained('microsoft/deberta-v3-base')\n",
        "\n",
        "        # Resize embeddings if vocab_size is provided\n",
        "        if vocab_size:\n",
        "            self.deberta.resize_token_embeddings(vocab_size)\n",
        "\n",
        "        self.classifier = nn.Linear(self.deberta.config.hidden_size, 2)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        cls_token = outputs.last_hidden_state[:, 0, :]\n",
        "        return self.classifier(cls_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpbcD7tDjDH0"
      },
      "outputs": [],
      "source": [
        "model = WSCClassifier().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "EPOCHS = 4\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    for batch in tqdm(train_loader):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(input_ids, attention_mask)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "\n",
        "    acc = correct / len(train_loader.dataset)\n",
        "    print(f\"Epoch {epoch+1} | Train Loss: {total_loss:.4f} | Train Acc: {acc*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpwP1PPIoFi8"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        logits = model(input_ids, attention_mask)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "\n",
        "val_acc = correct / len(val_loader.dataset)\n",
        "print(f\"✅ Validation Accuracy: {val_acc*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LKC2IshpkMQ"
      },
      "source": [
        "Span-Aware Fine-Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TI9rwtd_sAFJ"
      },
      "outputs": [],
      "source": [
        "!pip install transformers tqdm --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5SdW0FpvbHD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hm9XCbFTvhRC"
      },
      "outputs": [],
      "source": [
        "class SpanAwareWSCDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_len=128):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        example = self.data[idx]\n",
        "        sentence = example['text']\n",
        "        span1 = example['span1_text']\n",
        "        span2 = example['span2_text']\n",
        "        label = example['label']\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            sentence,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_len,\n",
        "            return_tensors='pt',\n",
        "            return_offsets_mapping=True\n",
        "        )\n",
        "\n",
        "        input_ids = encoding['input_ids'].squeeze()\n",
        "        attention_mask = encoding['attention_mask'].squeeze()\n",
        "        offset_mapping = encoding['offset_mapping'].squeeze()\n",
        "\n",
        "        span1_mask = torch.zeros_like(input_ids)\n",
        "        span2_mask = torch.zeros_like(input_ids)\n",
        "\n",
        "        span1_start = sentence.find(span1)\n",
        "        span1_end = span1_start + len(span1)\n",
        "        span2_start = sentence.find(span2)\n",
        "        span2_end = span2_start + len(span2)\n",
        "\n",
        "        for i, (start, end) in enumerate(offset_mapping):\n",
        "            if start == 0 and end == 0:\n",
        "                continue\n",
        "\n",
        "            if not (end <= span1_start or start >= span1_end):\n",
        "                span1_mask[i] = 1\n",
        "\n",
        "            if not (end <= span2_start or start >= span2_end):\n",
        "                span2_mask[i] = 1\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'span1_mask': span1_mask,\n",
        "            'span2_mask': span2_mask,\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyadDZdWv1jj"
      },
      "source": [
        "This model uses DeBERTa as a base encoder and adds:\n",
        "- Multi-head attention over each span\n",
        "- Cross-attention between the two spans\n",
        "- A classifier over the combined representations of `[CLS]`, span1, span2, and their interaction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6VEKLg3v3hd"
      },
      "outputs": [],
      "source": [
        "class SpanAwareWSCClassifier(nn.Module):\n",
        "    def __init__(self, model_name='microsoft/deberta-v3-base', hidden_size=768):\n",
        "        super().__init__()\n",
        "        self.deberta = AutoModel.from_pretrained(model_name)\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.span1_attention = nn.MultiheadAttention(hidden_size, 8, batch_first=True)\n",
        "        self.span2_attention = nn.MultiheadAttention(hidden_size, 8, batch_first=True)\n",
        "        self.cross_attention = nn.MultiheadAttention(hidden_size, 8, batch_first=True)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.layer_norm = nn.LayerNorm(hidden_size * 4)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_size * 4, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_size, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, span1_mask, span2_mask):\n",
        "        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_states = outputs.last_hidden_state\n",
        "        cls_repr = hidden_states[:, 0, :]\n",
        "\n",
        "        span1_repr = self.get_span_representation(hidden_states, span1_mask, self.span1_attention)\n",
        "        span2_repr = self.get_span_representation(hidden_states, span2_mask, self.span2_attention)\n",
        "        interaction_repr = self.get_interaction_representation(span1_repr, span2_repr)\n",
        "\n",
        "        combined = torch.cat([cls_repr, span1_repr, span2_repr, interaction_repr], dim=-1)\n",
        "        combined = self.layer_norm(combined)\n",
        "        combined = self.dropout(combined)\n",
        "        return self.classifier(combined)\n",
        "\n",
        "    def get_span_representation(self, hidden_states, span_mask, attention_layer, return_attention=False):\n",
        "        batch_size, seq_len, hidden_size = hidden_states.size()\n",
        "        span_attention_mask = span_mask.float()\n",
        "        has_span = span_attention_mask.sum(dim=1, keepdim=True) > 0\n",
        "\n",
        "        attn_output, attn_weights = attention_layer(\n",
        "            hidden_states,\n",
        "            hidden_states,\n",
        "            hidden_states,\n",
        "            key_padding_mask=(1 - span_attention_mask).bool(),\n",
        "            need_weights=True,\n",
        "            average_attn_weights=True\n",
        "        )\n",
        "\n",
        "        span_lengths = span_attention_mask.sum(dim=1, keepdim=True).clamp(min=1)\n",
        "        span_repr = (attn_output * span_attention_mask.unsqueeze(-1)).sum(dim=1) / span_lengths\n",
        "        cls_repr = hidden_states[:, 0, :]\n",
        "        final_repr = torch.where(has_span, span_repr, cls_repr)\n",
        "\n",
        "        if return_attention:\n",
        "            return final_repr, attn_weights\n",
        "        return final_repr\n",
        "\n",
        "    def get_interaction_representation(self, span1_repr, span2_repr):\n",
        "        interaction, _ = self.cross_attention(\n",
        "            span1_repr.unsqueeze(1),\n",
        "            span2_repr.unsqueeze(1),\n",
        "            span2_repr.unsqueeze(1)\n",
        "        )\n",
        "        return interaction.squeeze(1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8Ksc0giv9IA"
      },
      "source": [
        "This function handles the full training loop including:\n",
        "- Data loading\n",
        "- Accuracy and loss tracking\n",
        "- Learning rate scheduling\n",
        "- Model checkpointing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xguI5Nsv-md"
      },
      "outputs": [],
      "source": [
        "def train_span_aware_model(train_data, val_data, tokenizer, device, epochs=10, patience=3):\n",
        "    train_dataset = SpanAwareWSCDataset(train_data, tokenizer)\n",
        "    val_dataset = SpanAwareWSCDataset(val_data, tokenizer)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=8)\n",
        "\n",
        "    model = SpanAwareWSCClassifier().to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
        "    scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=epochs)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    print(f\"🚀 Model has {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
        "\n",
        "    best_val_acc = 0\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss, correct, total = 0, 0, 0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            span1_mask = batch['span1_mask'].to(device)\n",
        "            span2_mask = batch['span2_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(input_ids, attention_mask, span1_mask, span2_mask)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            preds = logits.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        val_loss, val_correct, val_total = 0, 0, 0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\"):\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                span1_mask = batch['span1_mask'].to(device)\n",
        "                span2_mask = batch['span2_mask'].to(device)\n",
        "                labels = batch['label'].to(device)\n",
        "\n",
        "                logits = model(input_ids, attention_mask, span1_mask, span2_mask)\n",
        "                val_loss += criterion(logits, labels).item()\n",
        "                preds = logits.argmax(dim=1)\n",
        "                val_correct += (preds == labels).sum().item()\n",
        "                val_total += labels.size(0)\n",
        "\n",
        "        train_acc = correct / total\n",
        "        val_acc = val_correct / val_total\n",
        "\n",
        "        print(f\"\\n📊 Epoch {epoch+1}: Train Acc = {train_acc*100:.2f}% | Val Acc = {val_acc*100:.2f}%\")\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), 'best_span_aware_wsc_model.pth')\n",
        "            print(\"✅ New best model saved!\")\n",
        "            patience_counter = 0  # reset counter\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"⚠️  No improvement. Patience: {patience_counter}/{patience}\")\n",
        "            if patience_counter >= patience:\n",
        "                print(\"🛑 Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    print(f\"\\n🏁 Training complete! Best Val Accuracy: {best_val_acc*100:.2f}%\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRNWxw95wYW_"
      },
      "outputs": [],
      "source": [
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-base')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = train_span_aware_model(train_data, val_data, tokenizer, device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoLP3Leixj5G"
      },
      "source": [
        "This utility helps visualize which tokens in the input contributed most to the model’s decision. We’ll use the attention weights from the span attention heads and CLS token relevance as a rough proxy (similar to Grad-CAM but adapted for transformers).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hj5e2qmixh81"
      },
      "outputs": [],
      "source": [
        "def visualize_attention(tokenizer, input_ids, span1_mask, span2_mask, attention_scores, title=\"Attention\"):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "    attention_scores = attention_scores.detach().cpu().numpy()\n",
        "\n",
        "    plt.figure(figsize=(len(tokens) * 0.3, 1.5))\n",
        "    sns.heatmap([attention_scores], xticklabels=tokens, cmap=\"Blues\", cbar=True, linewidths=0.5, annot=False)\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.title(title)\n",
        "    plt.yticks([])\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVHelktkxtxX"
      },
      "outputs": [],
      "source": [
        "def predict_and_visualize(model, tokenizer, sentence, span1_text, span2_text, device):\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize input\n",
        "    encoding = tokenizer(\n",
        "        sentence,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=128,\n",
        "        return_tensors='pt',\n",
        "        return_offsets_mapping=True\n",
        "    )\n",
        "\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "    offset_mapping = encoding['offset_mapping'].squeeze()\n",
        "\n",
        "    # Prepare span masks\n",
        "    span1_mask = torch.zeros_like(input_ids)\n",
        "    span2_mask = torch.zeros_like(input_ids)\n",
        "\n",
        "    s1_start = sentence.find(span1_text)\n",
        "    s1_end = s1_start + len(span1_text)\n",
        "    s2_start = sentence.find(span2_text)\n",
        "    s2_end = s2_start + len(span2_text)\n",
        "\n",
        "    for i, (start, end) in enumerate(offset_mapping):\n",
        "        if start == 0 and end == 0:\n",
        "            continue\n",
        "        if not (end <= s1_start or start >= s1_end):\n",
        "            span1_mask[0, i] = 1\n",
        "        if not (end <= s2_start or start >= s2_end):\n",
        "            span2_mask[0, i] = 1\n",
        "\n",
        "    # Forward pass with attention extraction\n",
        "    with torch.no_grad():\n",
        "        hidden_states = model.deberta(input_ids, attention_mask=attention_mask).last_hidden_state\n",
        "\n",
        "        s1_repr, s1_att = model.get_span_representation(hidden_states, span1_mask, model.span1_attention, return_attention=True)\n",
        "        s2_repr, s2_att = model.get_span_representation(hidden_states, span2_mask, model.span2_attention, return_attention=True)\n",
        "        interaction_repr = model.get_interaction_representation(s1_repr, s2_repr)\n",
        "        cls_repr = hidden_states[:, 0, :]\n",
        "\n",
        "        combined = torch.cat([cls_repr, s1_repr, s2_repr, interaction_repr], dim=-1)\n",
        "        combined = model.layer_norm(combined)\n",
        "        combined = model.dropout(combined)\n",
        "        logits = model.classifier(combined)\n",
        "\n",
        "        probs = F.softmax(logits, dim=-1).squeeze().cpu()\n",
        "        pred = torch.argmax(probs).item()\n",
        "        label_str = \"Same Entity\" if pred == 1 else \"Different Entities\"\n",
        "\n",
        "        print(f\"🔍 Sentence: {sentence}\")\n",
        "        print(f\"👤 Span1: \\\"{span1_text}\\\" | 👤 Span2: \\\"{span2_text}\\\"\")\n",
        "        print(f\"🧠 Prediction: {label_str} (Confidence: {probs[pred]:.2f})\")\n",
        "\n",
        "    visualize_attention(tokenizer, input_ids[0], span1_mask[0], span2_mask[0], s1_att[0].mean(0), title=\"Span 1 Attention\")\n",
        "    visualize_attention(tokenizer, input_ids[0], span1_mask[0], span2_mask[0], s2_att[0].mean(0), title=\"Span 2 Attention\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BUF-MYryUNW"
      },
      "outputs": [],
      "source": [
        "# Load model if not already in memory\n",
        "model.load_state_dict(torch.load('best_span_aware_wsc_model.pth', map_location=device))\n",
        "\n",
        "# Example interactive prediction\n",
        "predict_and_visualize(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    sentence=\"The trophy wouldn't fit in the suitcase because it was too big.\",\n",
        "    span1_text=\"trophy\",\n",
        "    span2_text=\"it\",\n",
        "    device=device\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load dataset and rename column\n",
        "dataset = load_dataset(\"super_glue\", \"wsc.fixed\")\n",
        "dataset = dataset.rename_column(\"label\", \"labels\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"SpanBERT/spanbert-large-cased\")\n",
        "\n",
        "# Insert special markers\n",
        "def insert_span_tokens(example):\n",
        "    text = example[\"text\"]\n",
        "    span1_start = example[\"span1_index\"]\n",
        "    span2_start = example[\"span2_index\"]\n",
        "    span1 = example[\"span1_text\"]\n",
        "    span2 = example[\"span2_text\"]\n",
        "\n",
        "    if span1_start < span2_start:\n",
        "        first, second = (span1_start, span1, \"[SPAN1]\", \"[/SPAN1]\"), (span2_start, span2, \"[SPAN2]\", \"[/SPAN2]\")\n",
        "    else:\n",
        "        first, second = (span2_start, span2, \"[SPAN2]\", \"[/SPAN2]\"), (span1_start, span1, \"[SPAN1]\", \"[/SPAN1]\")\n",
        "\n",
        "    def insert(text, start, span, start_token, end_token):\n",
        "        return text[:start] + start_token + span + end_token + text[start + len(span):]\n",
        "\n",
        "    text = insert(text, first[0], first[1], first[2], first[3])\n",
        "    text = insert(text, second[0] + len(first[2]) + len(first[3]), second[1], second[2], second[3])\n",
        "\n",
        "    example[\"input_text\"] = text\n",
        "    return example\n",
        "\n",
        "dataset = dataset.map(insert_span_tokens)\n",
        "\n",
        "# Tokenize\n",
        "def tokenize(example):\n",
        "    return tokenizer(example[\"input_text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize)\n"
      ],
      "metadata": {
        "id": "mzRdfqCnlyKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"SpanBERT/spanbert-large-cased\", num_labels=2)\n"
      ],
      "metadata": {
        "id": "lVhyRvAAmAvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./spanbert_wsc\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=4,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "def compute_metrics(p):\n",
        "    preds = np.argmax(p.predictions, axis=1)\n",
        "    return {\"accuracy\": accuracy_score(p.label_ids, preds)}\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "RwczN_rdmIJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = trainer.evaluate()\n",
        "print(f\"\\n📊 Final Validation Accuracy: {results['eval_accuracy']*100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "WbJUhUxynyZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_name = \"SpanBERT/spanbert-large-cased\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Pick a few validation examples\n",
        "samples = dataset[\"validation\"].select(range(5))\n",
        "\n",
        "def predict_and_print(example):\n",
        "    encoded = tokenizer(example[\"input_text\"], return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encoded)\n",
        "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "        pred = torch.argmax(probs, dim=-1).item()\n",
        "        print(f\"Text: {example['input_text']}\")\n",
        "        print(f\"Prediction: {'Coreferent' if pred == 1 else 'Not coreferent'}\")\n",
        "        print(f\"True Label: {'Coreferent' if example['labels'] == 1 else 'Not coreferent'}\")\n",
        "        print(f\"Confidence: {probs[0][pred]:.4f}\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "for ex in samples:\n",
        "    predict_and_print(ex)\n"
      ],
      "metadata": {
        "id": "YEW8MVqcpG-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_attention(model, tokenizer, text):\n",
        "    # Tokenize and prepare inputs\n",
        "    encoded = tokenizer(text, return_tensors=\"pt\", return_attention_mask=True)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(encoded[\"input_ids\"][0])\n",
        "\n",
        "    # Forward pass to extract attention\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encoded, output_attentions=True)\n",
        "\n",
        "    # Get attention from last layer, first head (CLS token to all)\n",
        "    attentions = outputs.attentions[-1][0][0]  # shape: (seq_len, seq_len)\n",
        "    cls_attention = attentions[0]  # CLS token\n",
        "\n",
        "    # Plot attention weights\n",
        "    plt.figure(figsize=(12, 2))\n",
        "    plt.bar(range(len(tokens)), cls_attention)\n",
        "    plt.xticks(range(len(tokens)), tokens, rotation=90)\n",
        "    plt.title(\"Attention weights from [CLS] token\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example\n",
        "sample_text = samples[0][\"input_text\"]\n",
        "show_attention(model, tokenizer, sample_text)\n"
      ],
      "metadata": {
        "id": "jeKAx0j4pP5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install bertviz\n",
        "from bertviz import head_view\n",
        "\n",
        "def interactive_attention(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    outputs = model(**inputs, output_attentions=True)\n",
        "    attention = outputs.attentions\n",
        "    head_view(attention, tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0]))\n",
        "\n",
        "# interactive_attention(sample_text)  # Uncomment in a notebook\n"
      ],
      "metadata": {
        "id": "UrKuIY9xpWpO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyM44teApfy6mWoziMrQMpLw",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}